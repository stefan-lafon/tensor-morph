{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGWqFyzsy9+QoCp5O01V1g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Peruk8BVyJYa","cellView":"form"},"outputs":[],"source":["# @title 1. Environment setup and data loading\n","import os\n","import sys\n","import importlib.util\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Project path configuration.\n","DRIVE_FOLDER = \"/content/drive/My Drive/projects/TensorMorph\"\n","LOCAL_FOLDER = \"/content/tensormorph_local\"\n","\n","# Mount Drive for data and schema access.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Workspace initialization.\n","os.makedirs(f\"{LOCAL_FOLDER}/experimental\", exist_ok=True)\n","os.makedirs(f\"{LOCAL_FOLDER}/data\", exist_ok=True)\n","\n","# Syncing local experimental scripts and the generated data.\n","print(\"Syncing files from Drive...\")\n","!rsync -av --progress \"{DRIVE_FOLDER}/experimental/\" \"{LOCAL_FOLDER}/experimental/\"\n","!cp -r \"{DRIVE_FOLDER}/data/\"* \"{LOCAL_FOLDER}/data/\"\n","\n","os.chdir(LOCAL_FOLDER)\n","\n","# Direct path import for schema.py to ensure alignment with DataGen.\n","schema_path = os.path.join(LOCAL_FOLDER, \"experimental/schema.py\")\n","spec = importlib.util.spec_from_file_location(\"schema\", schema_path)\n","schema = importlib.util.module_from_spec(spec)\n","try:\n","    spec.loader.exec_module(schema)\n","    global FEATURES, TARGET\n","    FEATURES = schema.FEATURES\n","    TARGET = schema.TARGET\n","    print(f\"Schema loaded: {len(FEATURES)} features identified.\")\n","except Exception as e:\n","    print(f\"Error: Failed to load schema.py: {e}\")\n","\n","# Load the datasets.\n","df_mem = pd.read_csv(\"data/dataset_memory_bound.csv\")\n","df_comp = pd.read_csv(\"data/dataset_compute_bound.csv\")\n","\n","print(f\"Loaded {len(df_mem)} Memory-Bound and {len(df_comp)} Compute-Bound samples.\")\n","print(\"Environment ready.\")"]},{"cell_type":"code","source":["# @title 2. Data preprocessing and feature engineering\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Constants for the 70/15/15 split.\n","TRAIN_RATIO = 0.7\n","VAL_RATIO = 0.15\n","TEST_RATIO = 0.15\n","SEED = 42\n","NORMALIZE = True\n","\n","def prepare_dataset(df, feature_cols, target_col):\n","    \"\"\"\n","    Splits data into train, validation, and test sets.\n","    \"\"\"\n","    X = df[feature_cols].values\n","    y = df[target_col].values\n","\n","    # Isolate the test set first.\n","    X_temp, X_test, y_temp, y_test = train_test_split(\n","        X, y, test_size=TEST_RATIO, random_state=SEED\n","    )\n","\n","    # Split the remainder into train and val.\n","    relative_val_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_temp, y_temp, test_size=relative_val_ratio, random_state=SEED\n","    )\n","\n","    scaler = None\n","    if NORMALIZE:\n","        scaler = StandardScaler()\n","        # Scale based on training distribution.\n","        X_train = scaler.fit_transform(X_train)\n","        X_val = scaler.transform(X_val)\n","        X_test = scaler.transform(X_test)\n","\n","    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n","\n","# Process both hardware targets.\n","res_mem = prepare_dataset(df_mem, FEATURES, TARGET)\n","X_train_mem, X_val_mem, X_test_mem, y_train_mem, y_val_mem, y_test_mem, scaler_mem = res_mem\n","\n","res_comp = prepare_dataset(df_comp, FEATURES, TARGET)\n","X_train_comp, X_val_comp, X_test_comp, y_train_comp, y_val_comp, y_test_comp, scaler_comp = res_comp\n","\n","# Print explicit counts.\n","print(f\"Memory split: {len(X_train_mem)} train, {len(X_val_mem)} val, {len(X_test_mem)} test.\")\n","print(f\"Compute split: {len(X_train_comp)} train, {len(X_val_comp)} val, {len(X_test_comp)} test.\")"],"metadata":{"id":"dQyW_rMRyiWZ","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Model training and evaluation\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Ensemble hyperparameters.\n","N_TREES = 100\n","LR = 0.1\n","DEPTH = 4\n","\n","def run_training(X_train, X_val, X_test, y_train, y_val, y_test, label):\n","    model = GradientBoostingRegressor(\n","        n_estimators=N_TREES,\n","        learning_rate=LR,\n","        max_depth=DEPTH,\n","        random_state=SEED\n","    )\n","\n","    # Fit the model.\n","    model.fit(X_train, y_train)\n","\n","    # Check validation performance.\n","    val_preds = model.predict(X_val)\n","    val_r2 = r2_score(y_val, val_preds)\n","\n","    # Unbiased final test.\n","    test_preds = model.predict(X_test)\n","    test_r2 = r2_score(y_test, test_preds)\n","    test_mse = mean_squared_error(y_test, test_preds)\n","\n","    print(f\"Results for {label}:\")\n","    print(f\"  Val R2:  {val_r2:.4f}\")\n","    print(f\"  Test R2: {test_r2:.4f}\")\n","    print(f\"  Test MSE: {test_mse:.4f}\\n\")\n","\n","    return model\n","\n","# Train the specialized advisors.\n","model_mem = run_training(\n","    X_train_mem, X_val_mem, X_test_mem, y_train_mem, y_val_mem, y_test_mem, \"Memory-Bound\"\n",")\n","\n","model_comp = run_training(\n","    X_train_comp, X_val_comp, X_test_comp, y_train_comp, y_val_comp, y_test_comp, \"Compute-Bound\"\n",")"],"metadata":{"id":"tH9NJfyey0Jv","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Feature importance and analysis\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_importance(model, feature_names, title, ax):\n","    # Extract importance scores.\n","    importances = model.feature_importances_\n","    indices = np.argsort(importances)\n","\n","    # Create horizontal bar chart.\n","    ax.barh(range(len(indices)), importances[indices], color='steelblue', align='center')\n","    ax.set_yticks(range(len(indices)))\n","    ax.set_yticklabels([feature_names[i] for i in indices])\n","    ax.set_title(title)\n","    ax.set_xlabel(\"Importance score\")\n","    ax.grid(axis='x', alpha=0.3)\n","\n","# Compare hardware profiles.\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","plot_importance(model_mem, FEATURES, \"Memory-Bound features\", ax1)\n","plot_importance(model_comp, FEATURES, \"Compute-Bound features\", ax2)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"xPuKtmVaDJpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Model persistence\n","import joblib\n","import os\n","\n","# Create local directory.\n","os.makedirs(\"models\", exist_ok=True)\n","\n","# Save Memory-Bound artifacts.\n","joblib.dump(model_mem, \"models/model_mem.joblib\")\n","joblib.dump(scaler_mem, \"models/scaler_mem.joblib\")\n","\n","# Save Compute-Bound artifacts.\n","joblib.dump(model_comp, \"models/model_comp.joblib\")\n","joblib.dump(scaler_comp, \"models/scaler_comp.joblib\")\n","\n","# Ensure Drive directory exists.\n","drive_models_path = f\"{DRIVE_FOLDER}/models\"\n","os.makedirs(drive_models_path, exist_ok=True)\n","\n","# Sync to Drive.\n","!cp -r models/* \"{drive_models_path}/\"\n","\n","print(f\"Models and scalers persisted to {drive_models_path}.\")"],"metadata":{"id":"ExiN6PLjEWqG"},"execution_count":null,"outputs":[]}]}