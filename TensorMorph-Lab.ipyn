{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "authorship_tag": "ABX9TyNtUCQ0uYzoi3idyQW9ThvG"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "REqmtqFLs-WA",
                "cellView": "form",
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# @title 1. Environment initialization\n",
                "# This cell mounts Drive and installs the LLVM/MLIR 18 toolchain.\n",
                "# Run this once per session.\n",
                "\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "# 1. Mount Drive\n",
                "if not os.path.exists(\"/content/drive\"):\n",
                "    drive.mount('/content/drive')\n",
                "\n",
                "# 2. Add LLVM 18 Repository & Install Toolchain\n",
                "# This handles the GPG keys and repo setup that standard apt misses.\n",
                "!wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -\n",
                "!echo \"deb http://apt.llvm.org/jammy/ llvm-toolchain-jammy-18 main\" | sudo tee /etc/apt/sources.list.d/llvm.list\n",
                "!apt-get update -y\n",
                "\n",
                "# Install headers, tools, and the build system\n",
                "!apt-get install -y libmlir-18-dev mlir-18-tools llvm-18-dev cmake ninja-build clang-18\n",
                "\n",
                "# 3. Set standard environment variables\n",
                "os.environ[\"CC\"] = \"clang-18\"\n",
                "os.environ[\"CXX\"] = \"clang++-18\"\n",
                "\n",
                "print(\"\\n--- Setup Complete ---\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 2. Sync source and build project\n",
                "import os\n",
                "\n",
                "DRIVE_FOLDER = \"/content/drive/My Drive/projects/TensorMorph\"\n",
                "LOCAL_FOLDER = \"/content/tensormorph_local\"\n",
                "\n",
                "%cd /content\n",
                "\n",
                "if os.path.exists(DRIVE_FOLDER):\n",
                "    print(\"Syncing files from Drive...\")\n",
                "    # Clean sync to ensure fresh build state.\n",
                "    !rm -rf {LOCAL_FOLDER}\n",
                "    !cp -r \"{DRIVE_FOLDER}\" {LOCAL_FOLDER}\n",
                "\n",
                "    os.chdir(LOCAL_FOLDER)\n",
                "    if not os.path.exists(\"build\"):\n",
                "        os.makedirs(\"build\")\n",
                "\n",
                "    os.chdir(\"build\")\n",
                "\n",
                "    # Build with debug symbols and MLIR/LLVM paths.\n",
                "    !cmake .. -G Ninja \\\n",
                "        -DMLIR_DIR=/usr/lib/llvm-18/lib/cmake/mlir \\\n",
                "        -DLLVM_DIR=/usr/lib/llvm-18/lib/cmake/llvm \\\n",
                "        -DCMAKE_BUILD_TYPE=Debug\n",
                "\n",
                "    !ninja\n",
                "\n",
                "    # Check for binary in build root (standard for top-level add_executable).\n",
                "    if os.path.exists(\"tensormorph-opt\"):\n",
                "        print(\"\\nBuild complete. Starting verification suite...\")\n",
                "        os.chdir(LOCAL_FOLDER)\n",
                "        # Execute the unified runner.\n",
                "        !python3 tests/run_tests.py\n",
                "    else:\n",
                "        print(\"\\n[ERROR] Build failed to produce tensormorph-opt binary.\")\n",
                "else:\n",
                "    print(f\"Directory not found: {DRIVE_FOLDER}\")"
            ],
            "metadata": {
                "id": "qlK73wr7tOrV",
                "cellView": "form",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1766902386437,
                    "user_tz": 480,
                    "elapsed": 54538,
                    "user": {
                        "displayName": "Stefan Lafon",
                        "userId": "17149650627927548318"
                    }
                },
                "outputId": "a8b9ea64-108f-4732-e261-21b181316174"
            },
            "execution_count": 29,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/content\n",
                        "Syncing files from Drive...\n",
                        "-- The C compiler identification is Clang 18.1.8\n",
                        "-- The CXX compiler identification is Clang 18.1.8\n",
                        "-- Detecting C compiler ABI info\n",
                        "-- Detecting C compiler ABI info - done\n",
                        "-- Check for working C compiler: /usr/bin/clang-18 - skipped\n",
                        "-- Detecting C compile features\n",
                        "-- Detecting C compile features - done\n",
                        "-- Detecting CXX compiler ABI info\n",
                        "-- Detecting CXX compiler ABI info - done\n",
                        "-- Check for working CXX compiler: /usr/bin/clang++-18 - skipped\n",
                        "-- Detecting CXX compile features\n",
                        "-- Detecting CXX compile features - done\n",
                        "-- Performing Test HAVE_FFI_CALL\n",
                        "-- Performing Test HAVE_FFI_CALL - Success\n",
                        "-- Found FFI: /usr/lib/x86_64-linux-gnu/libffi.so\n",
                        "-- Could NOT find LibEdit (missing: LibEdit_INCLUDE_DIRS LibEdit_LIBRARIES) \n",
                        "-- Performing Test Terminfo_LINKABLE\n",
                        "-- Performing Test Terminfo_LINKABLE - Success\n",
                        "-- Found Terminfo: /usr/lib/x86_64-linux-gnu/libtinfo.so\n",
                        "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\n",
                        "-- Found zstd: /usr/lib/x86_64-linux-gnu/libzstd.so\n",
                        "-- Found LibXml2: /usr/lib/x86_64-linux-gnu/libxml2.so (found version \"2.9.13\")\n",
                        "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
                        "-- Could NOT find LibEdit (missing: LibEdit_INCLUDE_DIRS LibEdit_LIBRARIES) \n",
                        "-- Using MLIRConfig.cmake in: /usr/lib/llvm-18/lib/cmake/mlir\n",
                        "-- Using LLVMConfig.cmake in: /usr/lib/llvm-18/lib/cmake/llvm\n",
                        "-- Configuring done (1.4s)\n",
                        "-- Generating done (0.0s)\n",
                        "-- Build files have been written to: /content/tensormorph_local/build\n",
                        "[8/8] Linking CXX executable tensormorph-opt\u001b[K\n",
                        "\n",
                        "Build complete. Starting verification suite...\n",
                        "========================================\n",
                        " TENSORMORPH TEST RUNNER\n",
                        "========================================\n",
                        "\n",
                        "[1/2] Running Flag Logic Tests...\n",
                        "  [OK] Transpose toggle\n",
                        "  [OK] Padding toggle\n",
                        "  [OK] Linear Math toggle\n",
                        "  [OK] Fan-out cloning\n",
                        "  [OK] AI Mock Approval logic\n",
                        "  [OK] AI Mock Veto logic\n",
                        "\n",
                        "[2/2] Running MLIR Regression Suite...\n",
                        "  [OK] complex_mobile_block.mlir\n",
                        "  [OK] conv_add_clamp.mlir\n",
                        "  [OK] conv_affine_fusion.mlir\n",
                        "  [OK] conv_mul_fusion.mlir\n",
                        "  [OK] conv_sub_fusion.mlir\n",
                        "  [OK] depthwise_fusion_suite.mlir\n",
                        "  [OK] fanout_cloning.mlir\n",
                        "  [OK] identity_add.mlir\n",
                        "  [OK] identity_mul.mlir\n",
                        "  [OK] identity_sub.mlir\n",
                        "  [OK] mega_fusion.mlir\n",
                        "  [OK] pad_elimination.mlir\n",
                        "  [OK] pointwise_add_chain.mlir\n",
                        "  [OK] pointwise_mul_chain.mlir\n",
                        "  [OK] transpose_fold.mlir\n",
                        "\n",
                        "========================================\n",
                        " SUCCESS: All 21 tests passed!\n",
                        "========================================\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 3. Test MLIR (Conv -> Mul -> Add -> Sub)\n",
                "%%writefile /content/tensormorph_local/test.mlir\n",
                "module {\n",
                "  func.func @main(%input: tensor<1x1024x1024x1xf32>) -> tensor<1x1024x1024x1xf32> {\n",
                "    %c_w = \"tosa.const\"() {value = dense<1.1> : tensor<1x1x1x1xf32>} : () -> tensor<1x1x1x1xf32>\n",
                "    %c_b = \"tosa.const\"() {value = dense<0.1> : tensor<1xf32>} : () -> tensor<1xf32>\n",
                "\n",
                "    %c_m1 = \"tosa.const\"() {value = dense<2.0> : tensor<1xf32>} : () -> tensor<1xf32>\n",
                "    %c_a1 = \"tosa.const\"() {value = dense<1.0> : tensor<1xf32>} : () -> tensor<1xf32>\n",
                "    %c_s1 = \"tosa.const\"() {value = dense<0.5> : tensor<1xf32>} : () -> tensor<1xf32>\n",
                "\n",
                "    // 1x1 Conv - Very low compute, high memory pressure\n",
                "    %0 = \"tosa.conv2d\"(%input, %c_w, %c_b) {\n",
                "      dilation = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 1, 1>\n",
                "    } : (tensor<1x1024x1024x1xf32>, tensor<1x1x1x1xf32>, tensor<1xf32>) -> tensor<1x1024x1024x1xf32>\n",
                "\n",
                "    %1 = \"tosa.mul\"(%0, %c_m1) {shift = 0 : i8} : (tensor<1x1024x1024x1xf32>, tensor<1xf32>) -> tensor<1x1024x1024x1xf32>\n",
                "    %2 = \"tosa.add\"(%1, %c_a1) : (tensor<1x1024x1024x1xf32>, tensor<1xf32>) -> tensor<1x1024x1024x1xf32>\n",
                "    %result = \"tosa.sub\"(%2, %c_s1) : (tensor<1x1024x1024x1xf32>, tensor<1xf32>) -> tensor<1x1024x1024x1xf32>\n",
                "\n",
                "    return %result : tensor<1x1024x1024x1xf32>\n",
                "  }\n",
                "}"
            ],
            "metadata": {
                "id": "JIRbH7HKSHEz",
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 4. Run TensorMorph optimizer\n",
                "print(\"Running TensorMorph with no AI advisor.\")\n",
                "!/content/tensormorph_local/build/tensormorph-opt \\\n",
                "    --tosa-opt \\\n",
                "    --allow-unregistered-dialect \\\n",
                "    /content/tensormorph_local/test.mlir\n",
                "\n",
                "print(\"Running TensorMorph with the memory AI advisor.\")\n",
                "!/content/tensormorph_local/build/tensormorph-opt \\\n",
                "    --tosa-opt=\"ai-advisor=memory min-profit=1.1 debug-ai\" \\\n",
                "    --allow-unregistered-dialect \\\n",
                "    /content/tensormorph_local/test.mlir\n",
                "\n",
                "print(\"Running TensorMorph with the compute AI advisor.\")\n",
                "!/content/tensormorph_local/build/tensormorph-opt \\\n",
                "    --tosa-opt=\"ai-advisor=compute min-profit=1.1 debug-ai\" \\\n",
                "    --allow-unregistered-dialect \\\n",
                "    /content/tensormorph_local/test.mlir"
            ],
            "metadata": {
                "id": "TVD4Y9TzSVVg",
                "collapsed": true,
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 5. Setup lowering utility\n",
                "with open(\"/content/tensormorph_local/lower.sh\", \"w\") as f:\n",
                "    f.write('''#!/bin/bash\n",
                "INPUT_MLIR=$1\n",
                "OUTPUT_LLVM=$2\n",
                "\n",
                "# The \"Safe\" Legalization Pipeline\n",
                "# This avoids macro-pipelines and manually manages the casting cleanup\n",
                "# to ensure indices are legalized correctly.\n",
                "/usr/lib/llvm-18/bin/mlir-opt $INPUT_MLIR \\\\\n",
                "    --allow-unregistered-dialect \\\\\n",
                "    --pass-pipeline=\"builtin.module( \\\\\n",
                "        func.func(tosa-to-linalg-named), \\\\\n",
                "        func.func(tosa-to-linalg), \\\\\n",
                "        func.func(tosa-to-arith), \\\\\n",
                "        reconcile-unrealized-casts, \\\\\n",
                "        one-shot-bufferize{allow-unknown-ops function-boundary-type-conversion=identity-layout-map}, \\\\\n",
                "        func.func(convert-linalg-to-loops), \\\\\n",
                "        func.func(convert-scf-to-cf), \\\\\n",
                "        expand-strided-metadata, \\\\\n",
                "        lower-affine, \\\\\n",
                "        reconcile-unrealized-casts, \\\\\n",
                "        convert-arith-to-llvm, \\\\\n",
                "        convert-math-to-llvm, \\\\\n",
                "        finalize-memref-to-llvm, \\\\\n",
                "        convert-func-to-llvm, \\\\\n",
                "        reconcile-unrealized-casts \\\\\n",
                "    )\" \\\\\n",
                "    -o $OUTPUT_LLVM\n",
                "''')\n",
                "\n",
                "!chmod +x /content/tensormorph_local/lower.sh\n",
                "print(\"Lowering utility script updated with multi-stage cast reconciliation.\")"
            ],
            "metadata": {
                "id": "eZkQ9BTNKA-0",
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 6. Generate execution wrapper\n",
                "with open(\"/content/tensormorph_local/benchmark_wrapper.mlir\", \"w\") as f:\n",
                "    f.write('''\n",
                "module {\n",
                "  // Global input initialized to 1.0 for consistent execution\n",
                "  memref.global \"private\" constant @input_data : memref<1x1024x1024x1xf32> = dense<1.0>\n",
                "\n",
                "  // Signature matches the 1x1 memory stress test case\n",
                "  func.func private @main(memref<1x1024x1024x1xf32>) -> memref<1x1024x1024x1xf32>\n",
                "\n",
                "  func.func @benchmark_entry() {\n",
                "    %in = memref.get_global @input_data : memref<1x1024x1024x1xf32>\n",
                "\n",
                "    // The call to @main processes 1 million pixels per iteration\n",
                "    %res = func.call @main(%in) : (memref<1x1024x1024x1xf32>) -> memref<1x1024x1024x1xf32>\n",
                "\n",
                "    return\n",
                "  }\n",
                "}\n",
                "''')\n",
                "print(\"Benchmark wrapper updated for 1024x1024 memory-bound stress test.\")"
            ],
            "metadata": {
                "id": "5YpbvkGQKDQq",
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# @title 7. Performance & structural benchmarking\n",
                "import subprocess\n",
                "import time\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "ROOT = \"/content/tensormorph_local\"\n",
                "os.makedirs(f\"{ROOT}/tests\", exist_ok=True)\n",
                "\n",
                "def count_linalg_ops(ir_content):\n",
                "    return ir_content.count(\"linalg.generic\")\n",
                "\n",
                "def create_extended_bench_ir(label, fused=False):\n",
                "    # Scale: 1x512x512x128 = ~134MB.\n",
                "    # Chain: 6 Pointwise operations.\n",
                "\n",
                "    # Generate the baseline: 6 separate sweeps through memory\n",
                "    baseline_ops = \"\"\n",
                "    for i in range(1, 7):\n",
                "        op_type = \"arith.addf\" if i % 2 != 0 else \"arith.mulf\"\n",
                "        in_buf = \"%arg0\" if i == 1 else f\"%m{i-1}\"\n",
                "        out_buf = f\"%m{i}\"\n",
                "        baseline_ops += f\"\"\"\n",
                "    linalg.generic {{indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]}} ins({in_buf} : memref<1x512x512x128xf32>) outs({out_buf} : memref<1x512x512x128xf32>) {{\n",
                "      ^bb0(%in: f32, %out: f32):\n",
                "        %res = {op_type} %in, %c1 : f32\n",
                "        linalg.yield %res : f32\n",
                "    }}\"\"\"\n",
                "\n",
                "    # Generate the optimized: 1 single sweep through memory\n",
                "    fused_ops = \"\"\"\n",
                "    linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%arg0 : memref<1x512x512x128xf32>) outs(%m6 : memref<1x512x512x128xf32>) {\n",
                "      ^bb0(%in: f32, %out: f32):\n",
                "        %t1 = arith.addf %in, %c1 : f32\n",
                "        %t2 = arith.mulf %t1, %c1 : f32\n",
                "        %t3 = arith.addf %t2, %c1 : f32\n",
                "        %t4 = arith.mulf %t3, %c1 : f32\n",
                "        %t5 = arith.addf %t4, %c1 : f32\n",
                "        %t6 = arith.mulf %t5, %c1 : f32\n",
                "        linalg.yield %t6 : f32\n",
                "    }\"\"\"\n",
                "\n",
                "    op_logic = fused_ops if fused else baseline_ops\n",
                "\n",
                "    content = f\"\"\"\n",
                "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n",
                "module {{\n",
                "  func.func @bench_core(%arg0: memref<1x512x512x128xf32>, %m1: memref<1x512x512x128xf32>, %m2: memref<1x512x512x128xf32>, %m3: memref<1x512x512x128xf32>, %m4: memref<1x512x512x128xf32>, %m5: memref<1x512x512x128xf32>, %m6: memref<1x512x512x128xf32>) {{\n",
                "    %c1 = arith.constant 1.1 : f32\n",
                "    {op_logic}\n",
                "    return\n",
                "  }}\n",
                "  func.func @benchmark_entry() {{\n",
                "    %m0 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m1 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m2 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m3 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m4 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m5 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    %m6 = memref.alloc() : memref<1x512x512x128xf32>\n",
                "    call @bench_core(%m0, %m1, %m2, %m3, %m4, %m5, %m6) : (memref<1x512x512x128xf32>, memref<1x512x512x128xf32>, memref<1x512x512x128xf32>, memref<1x512x512x128xf32>, memref<1x512x512x128xf32>, memref<1x512x512x128xf32>, memref<1x512x512x128xf32>) -> ()\n",
                "    memref.dealloc %m0 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m1 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m2 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m3 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m4 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m5 : memref<1x512x512x128xf32>\n",
                "    memref.dealloc %m6 : memref<1x512x512x128xf32>\n",
                "    return\n",
                "  }}\n",
                "}}\n",
                "\"\"\"\n",
                "    path = f\"{ROOT}/tests/heavy_{label}.mlir\"\n",
                "    with open(path, \"w\") as f: f.write(content)\n",
                "    return path, content\n",
                "\n",
                "def run_heavy_bench(label, is_fused):\n",
                "    ir_path, ir_content = create_extended_bench_ir(label, is_fused)\n",
                "    llvm_path = f\"{ROOT}/heavy_{label}.llvm.mlir\"\n",
                "    op_count = count_linalg_ops(ir_content)\n",
                "\n",
                "    try:\n",
                "        pipeline = \"builtin.module(func.func(convert-linalg-to-loops),convert-scf-to-cf,convert-arith-to-llvm,finalize-memref-to-llvm,convert-func-to-llvm,reconcile-unrealized-casts)\"\n",
                "        subprocess.run([\"/usr/lib/llvm-18/bin/mlir-opt\", ir_path, f\"--pass-pipeline={pipeline}\", \"-o\", llvm_path], check=True)\n",
                "\n",
                "        cmd = [\"/usr/lib/llvm-18/bin/mlir-cpu-runner\", llvm_path, \"-e\", \"benchmark_entry\", \"-entry-point-result=void\", \"-shared-libs=/usr/lib/llvm-18/lib/libmlir_c_runner_utils.so\"]\n",
                "\n",
                "        latencies = []\n",
                "        print(f\"Running {label} (Ops: {op_count})...\")\n",
                "        for _ in range(3): subprocess.run(cmd, capture_output=True) # Warmup\n",
                "        for _ in range(15):\n",
                "            start = time.perf_counter()\n",
                "            subprocess.run(cmd, capture_output=True)\n",
                "            latencies.append((time.perf_counter() - start) * 1000)\n",
                "\n",
                "        return np.mean(latencies), op_count\n",
                "    finally:\n",
                "        if os.path.exists(ir_path): os.remove(ir_path)\n",
                "        if os.path.exists(llvm_path): os.remove(llvm_path)\n",
                "\n",
                "# EXECUTE\n",
                "t_base, o_base = run_heavy_bench(\"baseline\", False)\n",
                "t_opt, o_opt = run_heavy_bench(\"optimized\", True)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"       TENSORMORPH DEEP CHAIN RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Tensor Size:      1x512x512x128 (~134MB)\")\n",
                "print(f\"Baseline Ops:     {o_base}\")\n",
                "print(f\"Optimized Ops:    {o_opt}\")\n",
                "print(f\"Kernel Reduction: {o_base - o_opt} kernels removed\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Baseline:         {t_base:.2f} ms\")\n",
                "print(f\"Optimized:        {t_opt:.2f} ms\")\n",
                "print(f\"SPEEDUP:          {t_base/t_opt:.2f}x\")\n",
                "print(f\"BANDWIDTH SAVED:  ~{(t_base - t_opt) / t_base * 100:.1f}%\")\n",
                "print(\"=\"*50)"
            ],
            "metadata": {
                "id": "RYiSgdBzKE4K",
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}